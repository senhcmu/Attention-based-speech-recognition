# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iIj1Y9Dvei8MZAan5LwrcdjJfXKVk2F-
"""

# Commented out IPython magic to ensure Python compatibility.
# #!/usr/bin/env python
# # coding: utf-8

# # **Mount Drive**
# # 
# #   People not using Colab can delete this block


# % pip install python-levenshtein

print("after install")


# from google.colab import drive
# drive.mount('/content/drive')





# #Making the current directory as the directory where all the files are.
# get_ipython().run_line_magic('cd', './drive/"My Drive"/Files')




from matplotlib.lines import Line2D
import matplotlib.pyplot as plt
import numpy as np
import torch 
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.nn.utils as utils
import pickle as pk
import random
from torch.utils.data import DataLoader, Dataset 
import time
from torch.nn.utils.rnn import *
import Levenshtein
device = 'cuda' if torch.cuda.is_available() else 'cpu'
from torchnlp.nn import LockedDropout

print(device)


# # **Load data**
# 
# Loading all the numpy files containing the utterance information and text information


speech_train = np.load('train_new.npy', allow_pickle=True, encoding='bytes')
speech_valid = np.load('dev_new.npy', allow_pickle=True, encoding='bytes')
speech_test = np.load('test_new.npy', allow_pickle=True, encoding='bytes')

transcript_train = np.load('train_transcripts.npy', allow_pickle=True,encoding='bytes')
transcript_valid = np.load('dev_transcripts.npy', allow_pickle=True,encoding='bytes')
print("Data Loading Sucessful.....")


# # **Transform Text Data**
# 
# `transform_letter_to_index` function transforms alphabetical input to numerical input. Each letter is replaced by its corresponding index from `letter_list` .


# letter_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<sos>','<eos>'] #34 
letter_list = ['<sos>','A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', "'", '.', '_', '+', ' ','<eos>'] #34 

# dic = {}
# for character in range(len(letter_list)):

def writeFile(path, contents):
    with open(path, "wt") as f:
        f.write(contents)

def transform_letter_to_index(transcript, letter_list):
    '''
    :param transcript :(N, ) Transcripts are the text input
    :param letter_list: Letter list defined above
    :return letter_to_index_list: Returns a list for all the transcript sentence to index
    '''

    print(transcript.shape)
    print(transcript[0][0])
    result = []
    for sentence in transcript:
      temp = [1]
      for word in sentence:
        for letter in word.decode('UTF-8'):
          temp.append(letter_list.index(letter)+1)
        temp.append(33)
      temp = temp[:-1]
      temp.append(34)
      result.append(temp)


    return result







character_text_train = transform_letter_to_index(transcript_train, letter_list)
character_text_valid = transform_letter_to_index(transcript_valid, letter_list)
print("Transformed data sucessfully.....")
print(character_text_train[0])
print(character_text_valid[0])
# 
# # **Pyramidal BiLSTM**
#  
# 
# *   The length of utterance (speech input) can be hundereds to thousands of frames long.
# *   Paper reports that that a direct LSTM implementation as Encoder resulted in slow convergence and inferior results even after extensive training.
# *   The major reason is inability of `AttendAndSpell` operation to extract relevant information from a large number of input steps.


class pBLSTM(nn.Module):

  def __init__(self, input_dim, hidden_dim):
      super(pBLSTM, self).__init__()
      self.blstm = nn.LSTM(input_size=input_dim*2,hidden_size=hidden_dim,num_layers=1,bidirectional=True)
  def forward(self,x,lens):
    '''
    :param x :(N,T) input to the pBLSTM
    :return output: (N,T,H) encoded sequence from pyramidal Bi-LSTM 
    '''
    # print(len(x[0])) # 256
    # print(len(x)) #41507

    # x, lens = pad_packed_sequence(x)
    x = x.transpose(0,1)
    dimension = len(x[0][0])#256
    t = len(x[0])
    batch_size = len(x)#64

    if t%2 != 0:
      pad = torch.zeros(batch_size, 1, dimension).to(device)

      x = torch.cat((x,pad),dim=1)
      t += 1
    out = x.reshape(batch_size,t//2,dimension*2)
    lens = lens//2
    out = out.transpose(0,1)

    out = pack_padded_sequence(out, lengths=lens, batch_first=False, enforce_sorted=False)

    out = self.blstm(out)[0]
    out, lens = pad_packed_sequence(out)

    # out, lens = pad_packed_sequence(out)

    return out, lens

# # **Encoder**
# 
# *    Encoder takes the utterances as inputs and returns the key and value.
# *    Key and value are nothing but simple projections of the output from pBLSTM network.


class Encoder(nn.Module):
  def __init__(self, input_dim, hidden_dim, value_size=128,key_size=128):
    super(Encoder, self).__init__()
    self.lstm = nn.LSTM(input_size=input_dim,hidden_size=hidden_dim,num_layers=1,bidirectional=True)
    self.pBLSTM1 = pBLSTM(hidden_dim*2,hidden_dim)
    self.pBLSTM2 = pBLSTM(hidden_dim*2,hidden_dim)
    self.pBLSTM3 = pBLSTM(hidden_dim*2,hidden_dim)
    self.lockeddropout = LockedDropout(0.3)
    #Here you need to define the blocks of pBLSTMs

    self.key_network = nn.Linear(hidden_dim*2, value_size)
    self.value_network = nn.Linear(hidden_dim*2, key_size)
  
  def forward(self,x, lens):
    rnn_inp = pack_padded_sequence(x, lengths=lens, batch_first=False, enforce_sorted=False)
    # print('after pack:',rnn_inp[0].shape) # 41597 40
    outputs, _ = self.lstm(rnn_inp)
    outputs, lens = pad_packed_sequence(outputs)
    # print('after first lstm:',outputs[0].shape) # 41507 256

    outputs,lens =  self.pBLSTM1(outputs,lens)
    outputs = self.lockeddropout(outputs)

    outputs, lens =  self.pBLSTM2(outputs,lens)
    outputs = self.lockeddropout(outputs)

    linear_input, output_lens  =  self.pBLSTM3(outputs, lens)
    linear_input = self.lockeddropout(linear_input)

    #Use the outputs and pass it through the pBLSTM blocks
    # linear_input, output_lens = utils.rnn.pad_packed_sequence(outputs)
    keys = self.key_network(linear_input)
    value = self.value_network(linear_input)

    return keys, value, output_lens


# # **Attention**
# 
# *    Attention is calculated using key, value and query from Encoder and decoder.
# 
# Below are the set of operations you need to perform for computing attention.
# 
# ```
# energy = bmm(key, query)
# attention = softmax(energy)
# context = bmm(attention, value)
# ```
# 
# 


class Attention(nn.Module):
  def __init__(self):
    super(Attention, self).__init__()
    self.softmax = nn.Softmax(dim=-1)
  def forward(self, query, key, value, lens):
    '''
    :param query :(N,context_size) Query is the output of LSTMCell from Decoder
    :param key: (N,key_size) Key Projection from Encoder per time step
    :param value: (N,value_size) Value Projection from Encoder per time step
    :return output: Attended Context
    :return attention_mask: Attention mask that can be plotted  
    '''
    # print("attention key size:",key.shape) #(146,64,128)
    key = key.transpose(0,1)
    value = value.transpose(0,1)
    # print('key size:',key.shape)
    # print('query size:',query.shape) #(64,128)

    energy = torch.bmm(key, query.unsqueeze(2)).squeeze(2)
    # energy = torch.bmm(key, query.unsqueeze(2))
    # print('energy size:', energy.shape) # (64, 203)
    attention = self.softmax(energy)

    # print('lens size:',lens.shape) # (64, 128)
    mask = torch.zeros(attention.size()).to(device)

    for i in range(len(lens)):
      mask[i,:lens[i]] = 1
    mask = mask.to(device)
    attention = mask*attention
    # attention = attention.contiguous()

    # attention.masked_fill_(mask, -1e9)
    # print('attention shape:',attention.shape)

    attention = nn.functional.normalize(attention, dim=-1,p=1)

    # print('attention shape:',attention.shape)

    # print('value size:', value.shape)
    # print('attention size:', attention.shape)



    output = torch.bmm(attention.unsqueeze(1), value).squeeze(1)
    return output, attention




# # **Decoder**
# 
# *    As mentioned in Recitation-9 each forward call of decoder deals with just one time step. Thus we use LSTMCell instead of LSLTM here.
# *    Output from the second LSTMCell can be used as query here for attention module.
# *    In place of `value` that we get from the attention, this can be replace by context we get from the attention.
# *    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.


class Decoder(nn.Module):
  def __init__(self, vocab_size, hidden_dim, value_size=128, key_size=128,  isAttended=False):
    super(Decoder, self).__init__()
    self.embedding = nn.Embedding(vocab_size+1, key_size+value_size)
    # self.embedding = nn.Embedding(vocab_size+1, value_size)
    
    self.lstm1 = nn.LSTMCell(input_size=value_size+key_size+value_size, hidden_size=hidden_dim)
    self.lstm2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=key_size)
    self.isAttended = isAttended
    if(isAttended):
      self.attention = Attention()
    self.projectionForQuery = nn.Linear(key_size,key_size)
    # self.projectionForQueryTanh = nn.Tanh()
    self.character_prob = nn.Linear(key_size+value_size,vocab_size+1)
    self.embedding.weight = self.character_prob.weight

  def forward(self, key, values, lens, train=False, validation=False, text=None):
    '''
    :param key :(T,N,key_size) Output of the Encoder Key projection layer
    :param values: (T,N,value_size) Output of the Encoder Value projection layer
    :param text: (N,text_len) Batch input of text with text_length
    :param train: Train or eval mode
    :return predictions: Returns the character perdiction probability 
    '''
    # print("key size:",key.shape)
    batch_size = key.shape[1]
    if(train):
      # print("key size:",key.shape) #(154, 64, 128)
      # print("text shape:",text.shape) #(64,201)
      max_len = text.shape[1]
      # print('max len:', max_len)
      embeddings = self.embedding(text)
    elif validation:
      max_len = text.shape[1]
      # print('max len:', max_len)
      embeddings = self.embedding(text)
      max_len_validation = 250

    else:
      max_len = 250
    
    predictions = []
    hidden_states = [None, None]
    prediction = torch.ones(batch_size,1).to(device)
    # context = torch.zeros(values.shape[1], values.shape[2]).to(device)
    query = torch.zeros(batch_size, key.shape[-1]).to(device)

    for i in range(max_len):
      '''
      Here you should implement Gumble noise and teacher forcing techniques
      '''
      if(train):
        ran = random.randint(1,100)
        if teacherForcingRate >= ran:
          char_embed = embeddings[:,i,:]
        else:
          char_embed = self.embedding(prediction.argmax(dim=-1))
        
      else:
        char_embed = self.embedding(prediction.argmax(dim=-1))
      #When attention is True you should replace the values[i,:,:] with the context you get from attention

      if self.isAttended:
        context, attention_mask = self.attention(query, key, values, lens)


      inp = torch.cat([char_embed,context], dim=1)

      hidden_states[0] = self.lstm1(inp,hidden_states[0])
      
      inp_2 = hidden_states[0][0]
      hidden_states[1] = self.lstm2(inp_2,hidden_states[1])

      query = hidden_states[1][0]
      query = self.projectionForQuery(query)
      # output = self.projectionForQueryTanh(output)

      # prediction = self.character_prob(torch.cat([output, values[i,:,:]], dim=1))
      prediction = self.character_prob(torch.cat([context, hidden_states[1][0]], dim=1))
      # print("prediction:",prediction)
      # print("inside decoder forward:",prediction.shape) #(64,34)
      predictions.append(prediction.unsqueeze(1))


    if validation:
      predictions_validation_2 = []
      hidden_states = [None, None]
      # prediction = torch.ones(batch_size,1).to(device)
      prediction = torch.ones(batch_size,1).to(device)
      # context = torch.zeros(values.shape[1], values.shape[2]).to(device)
      query = torch.zeros(batch_size,key.shape[-1]).to(device)

      for i in range(max_len_validation):
        '''
        Here you should implement Gumble noise and teacher forcing techniques
        '''
        
        char_embed = self.embedding(prediction.argmax(dim=-1))
        #When attention is True you should replace the values[i,:,:] with the context you get from attention
        if self.isAttended:
          context, attention_mask = self.attention(query, key, values, lens)



        inp = torch.cat([char_embed,context], dim=1)

        hidden_states[0] = self.lstm1(inp,hidden_states[0])

        
        inp_2 = hidden_states[0][0]
        hidden_states[1] = self.lstm2(inp_2,hidden_states[1])

        query = hidden_states[1][0]
        query = self.projectionForQuery(query)



        # prediction = self.character_prob(torch.cat([query, values[i,:,:]], dim=1))
        prediction = self.character_prob(torch.cat([context, hidden_states[1][0]], dim=1))
        # print("prediction:",prediction)
        # print("inside decoder forward:",prediction.shape) #(64,34)
        predictions_validation_2.append(prediction.unsqueeze(1))

    if not validation:
      return torch.cat(predictions, dim=1)
    elif validation:
      return torch.cat(predictions, dim=1), torch.cat(predictions_validation_2, dim=1)



# # **Sequence to Sequence Model**
# 
# *    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.

# In[ ]:


class Seq2Seq(nn.Module):
  def __init__(self,input_dim,vocab_size,hidden_dim,value_size=128, key_size=128,isAttended=False):
    super(Seq2Seq,self).__init__()

    self.encoder = Encoder(input_dim, hidden_dim)
    self.decoder = Decoder(vocab_size, hidden_dim, isAttended=True)
  def forward(self,speech_input, speech_len, text=None, train=False, validation=False):
    key, value, lens = self.encoder(speech_input, speech_len)
    # print("key:",key)
    # print("value:",value)
    if(train):
      predictions = self.decoder(key, value, lens, text=text, train=True)
    elif validation:
      # predictions = self.decoder(key, value, lens, text=None, train=False)
      predictions = self.decoder(key, value, lens, text=text, train=False,validation=True)
    else:
      predictions = self.decoder(key, value, lens, train=False,validation=False)
    return predictions


# # **DataLoader**
# 
# Below is the dataloader for the homework.
# 
# *    You are expected to fill in the collate function if you use this code skeleton.

# In[ ]:


class Speech2Text_Dataset(Dataset):
  def __init__(self, speech, text=None, train=True):
    self.speech = speech
    self.train = train
    if(text is not None):
      self.text = text
  def __len__(self):
    return self.speech.shape[0]
  def __getitem__(self, index):
    if(self.train):
      
      return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(self.text[index])
    else:
      return torch.tensor(self.speech[index].astype(np.float32))


# In[ ]:


def collate_train(batch_data):
  '''
  Complete this function.
  I usually return padded speech and text data, and length of 
  utterance and transcript from this function 
  '''
  # print(batch_data[0][0].shape[0],batch_data[0][0].shape[1])  # 550 40
  # print(len(batch_data)) # 64
  # print(batch_data[0][0].shape) # 537 40




  utterance = pad_sequence([s[0] for s in batch_data])
  # print(len(utterance)) #1431
  # print(utterance[0].shape) #64 40

  # print(utterance[0].shape[0],utterance[0].shape[1])  # 64 40


  utterance_lens = torch.LongTensor([len(s[0]) for s in batch_data])

  transcript = [s[1] for s in batch_data]

  transcript = pad_sequence(transcript, batch_first=True)

  transcript_lens = torch.LongTensor([len(s[1]) for s in batch_data])
  return utterance, transcript, utterance_lens, transcript_lens

def collate_test(batch_data):
  '''
  Complete this function.
  I usually return padded speech and length of 
  utterance from this function 
  '''
  utterance = pad_sequence([s for s in batch_data])

  utterance_lens = torch.LongTensor([len(s) for s in batch_data])

  return utterance, utterance_lens

Speech2Text_train_Dataset = Speech2Text_Dataset(speech_train, character_text_train)
Speech2Text_validation_Dataset = Speech2Text_Dataset(speech_valid, character_text_valid)
Speech2Text_test_Dataset = Speech2Text_Dataset(speech_test, None, False)

train_loader = DataLoader(Speech2Text_train_Dataset, batch_size=64, shuffle=True, collate_fn=collate_train)
validation_loader = DataLoader(Speech2Text_validation_Dataset, batch_size=64, shuffle=True, collate_fn=collate_train)
test_loader = DataLoader(Speech2Text_test_Dataset, batch_size=64, shuffle=False, collate_fn=collate_test)


# # **Learning**
# 
# Defining the Sequence to Sequence model, optimizer and criterion for learning.
# 
# Train routine is also provided here which can be referenced while writing validation and test routine.

model = Seq2Seq(input_dim=40,vocab_size=len(letter_list),hidden_dim=512)

checkpoint = torch.load('try18epochesbackupdis20.pth')
model.load_state_dict(checkpoint['model_state_dict'])

# model.load()
model = model.to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

for state in optimizer.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor):
            state[k] = v.to(device)

criterion = nn.CrossEntropyLoss(reduction='none').to(device)

'''
If you plan to use the code skeleton from the
recitation pass model.parameters() from train loop.
model here is an object of Seq2Seq class.
'''

def plot_grad_flow(named_parameters):
    '''Plots the gradients flowing through different layers in the net during training.
    Can be used for checking for possible gradient vanishing / exploding problems.
    
    Usage: Plug this function in Trainer class after loss.backwards() as 
    "plot_grad_flow(self.model.named_parameters())" to visualize the gradient flow'''
    ave_grads = []
    max_grads= []
    layers = []
    for n, p in named_parameters:
        if(p.requires_grad) and ("bias" not in n):
            if(p is not None):
                layers.append(n)
                ave_grads.append(p.grad.abs().mean())
                max_grads.append(p.grad.abs().max())
    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color="c")
    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color="b")
    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color="k" )
    plt.xticks(range(0,len(ave_grads), 1), layers, rotation="vertical")
    plt.xlim(left=0, right=len(ave_grads))
    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions
    plt.xlabel("Layers")
    plt.ylabel("average gradient")
    plt.title("Gradient flow")
    #plt.tight_layout()
    plt.grid(True)
    plt.legend([Line2D([0], [0], color="c", lw=4),
                Line2D([0], [0], color="b", lw=4),
                Line2D([0], [0], color="k", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])
    return plt



def train(model,train_loader, validation_loader, num_epochs, criterion, optimizer, letter_list):
  global teacherForcingRate
  flag = False
  for epochs in range(num_epochs):
    model.train()

    print("%d Epoches:" % epochs)
    loss_sum = 0
    batch_id = 0
    trainSince = time.time()
    if epochs == 3:
      flag = True
    for (batch_num, collate_output) in enumerate(train_loader):
      optimizer.zero_grad()
      batch_id += 1
      with torch.autograd.set_detect_anomaly(True):
        
        speech_input, text_input, speech_len, text_len = collate_output
        speech_input = speech_input.to(device)
        text_input = text_input.to(device)
        # print("check text_input size 1:", text_input.shape)

        predictions = model(speech_input, speech_len ,text=text_input[:,:-1], train=True)
        # print("train:",predictions[0].argmax(dim=-1))
        mask = torch.zeros(text_input.size()).to(device)

        for i in range(len(text_len)):
          mask[i,:text_len[i]-1] = 1

        mask = mask[:,:-1].contiguous()
        mask = mask.view(-1).to(device)
        

        predictions = predictions.contiguous().view(-1, predictions.size(-1))
        # print("check text_input size 2:", text_input.shape)
        truncate = text_input[:,1:]
        truncate = truncate.contiguous().view(-1)
        # print("check text_input size 3:", text_input.shape)

        loss = criterion(predictions, truncate)
        masked_loss = torch.sum(loss*mask)

        masked_loss.backward()

        torch.nn.utils.clip_grad_norm(model.parameters(), 2)
        optimizer.step()


        current_loss = float(torch.sum(masked_loss).item())/int(torch.sum(mask).item())
        # print("current loss:",current_loss)
        loss_sum += current_loss
        # print("loss:",loss_sum)


    if flag:
      teacherForcingRate -= 3
      if teacherForcingRate <= 50:
        flag = False

    loss_sum = loss_sum / batch_id
    training_perplexity = np.exp(loss_sum)
    trainEnd = time.time()
    valiSince = time.time()
    validation_loss, validation_perplexity, distance = validation(model, validation_loader, criterion, letter_list)
    valiEnd = time.time()

    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
        }, "greedysearch6.pth")
    print(model.parameters())
    plot_grad_flow(model.named_parameters())
    now = time.time()
    print("train time:",trainEnd-trainSince)
    print("validation time:",valiEnd-valiSince)
    print('Train Loss: {:.4f}\tTrain Perplexity: {:.4f}\tVal Loss: {:.4f}\tVal Perplexity: {:.4f}\t distance: {:.4f}'.
                  format(loss_sum, training_perplexity, validation_loss, validation_perplexity, distance))




def validation(model, validation_loader, criterion, letter_list):
  model.eval()
  loss_sum = 0
  # since = time.time()
  batch_id = 0
  distance = 0
  
  for (batch_num, collate_output) in enumerate(validation_loader):
    batch_id += 1
    with torch.autograd.set_detect_anomaly(True):
      
      speech_input, text_input, speech_len, text_len = collate_output
      speech_input = speech_input.to(device)
      text_input = text_input.to(device)
      # print("check text_input size 1:", text_input.shape)

      predictions, predictions_for_distance = model(speech_input, speech_len, text=text_input[:,:-1], train=False, validation=True)

      unpadded_text_input = []
      for sentence in range(len(text_input)):
        temp = ""

        transfer = text_input[sentence][1:text_len[sentence]]
        for character in transfer:
          if character == 0:
            continue
          if character == 1:
            continue

          elif character == 34:
            break
          temp += letter_list[character-1]
        unpadded_text_input.append(temp)
        

      batch_distance = 0
      for sentence in range(len(predictions_for_distance)):
        temp = ""
        # print("eos:",predictions_for_distance[sentence])
        # print(len(predictions_for_distance[sentence][0]))
        # print(torch.sum(predictions_for_distance[sentence][0]))
        eos = predictions_for_distance[sentence].argmax(dim=-1)
        # print("eos:",eos)
        for character in eos:
          if character == 0:
            continue
          if character == 1:
            continue
          if character == 34:
            break
          temp += letter_list[character-1]
        # print("what is this character:",character)
        # print("lens of prediction:", len(temp))
        # prin
        
        print("prediction output:",temp)
        batch_distance += (Levenshtein.distance(temp, unpadded_text_input[sentence]))
      distance += (batch_distance/len(predictions_for_distance))



      

      mask = torch.zeros(text_input.size()).to(device)

      for i in range(len(text_len)):
        mask[i,:text_len[i]-1] = 1

      mask = mask[:,:-1].contiguous()
      
      mask = mask.view(-1).to(device) 
      # print('check predictions size1:', predictions.shape) #(64,250,34)
      
      predictions = predictions.contiguous().view(-1, predictions.size(-1))
      # print('check predictions size2:', predictions.shape) 
      # print("check text_input size 2:", text_input.shape) #(64,200)
      truncate = text_input[:,1:]
      truncate = truncate.contiguous().view(-1)
      # print("check text_input size 3:", text_input.shape)

      loss = criterion(predictions, truncate)
      masked_loss = torch.sum(loss*mask)

      current_loss = float(torch.sum(masked_loss).item())/int(torch.sum(mask).item())
      loss_sum += current_loss
      # break

  distance = distance/batch_id

  loss_sum = loss_sum/batch_id
  validation_perplexity = np.exp(loss_sum)


  

  return loss_sum, validation_perplexity, distance

      # if  batch_num % 25 == 1:
      #   print('validation_loss', current_loss)

def test(model,test_loader, letter_list):
  model.eval()
  result = "Id,Predicted\n"
  num = 1
  for (batch_num, collate_output) in enumerate(test_loader):
    with torch.autograd.set_detect_anomaly(True):
      
      speech_input, speech_len = collate_output
      speech_input = speech_input.to(device)

      predictions = model(speech_input,speech_len, train=False)
      
      for sentence in range(len(predictions)):
        # num += 1
        result += str(num)
        result += ","
        temp = ""
        eos = predictions[sentence].argmax(dim=-1)
        for character in eos:
          if character == 1:
            continue
          if character == 0:
            continue
          if character == 34:
            break
          temp += letter_list[character-1]
        result += temp
        result += "\n"
        num += 1

  return result





teacherForcingRate = 70
train(model, train_loader, validation_loader, 55, criterion, optimizer, letter_list)
result = test(model, test_loader, letter_list)


writeFile("output7.csv", result)

# predictions = test(model, test_loader)


# # ***Moving Forward....***
# 
# We have provided a skeleton to begin with, so that you have a clear picture when writing the code. Apart from the methods given here following methods should be implemented.
#   
# *    Validation and test methods.
# *    Methods to convert indexes to characters
# *    Methods for calculating the perplexity/Levenstine distance for gauging the training routine.
# *    For visualizing the gradient flow (refer to FAQs) and attention graph (refer to Recitation-9) methods are already given.

# **Thank You !!** 
# 
# HW3 is due on Saturday 11/09/2019. To solve your last minute queries, **Amit** is having a full day OH **(from 12:00 pm to 6:00 pm)** this Saturday (piazza post very soon....)
# 
# *As always the secret to finish on time is to start early :-) ~ Amit*
